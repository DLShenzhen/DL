
Chapter 5.1 and 5.2

[1]：关于P63，脑子里需要有这个概念：
“大部分算法都有超参数（必须在学习算法外设定），我们需探讨如何使用额外的数据设置超参数。”
我：这里提到的额外数据，我认为是指对数据分布等数据特征的理解。

[2]：关于P63：
“大部分深度学习算法都是基于被称为随机梯度下降的算法求解的”
我：注意这句话

[3]：关于P63：
“通常机器学习任务定义为机器学习系统应该如何处理样本”
我：这句话是否说明机器学习实际是case by case算法，因为样本是千变万化的

[4]：关于P65，结构化输出：
“图像的像素级分割，将每一个像素分配到特定类别。例如，深度学习可用于标注航拍照片中的道路位置。在这些标注型的任务中，输出的结构形式不需要和输入尽可能相似。例如，在为图片添加描述的任务中，计算机程序观察到一幅图，输出描述这幅图的自然语言句子。这类任务被称为结构化输出任务，因为输出值之间内部紧密相关”
我：对结构化输出更深入了解，要注意两个应用场景：像素级标注和NLP

[5]：关于P65，合成采样：
“在这类任务中，机器学习程序生产一些和训练数据相似的新样本”
我：理解的不是很透彻，是指先学习出分布函数，再生成样本？需要一个更实际的例子来理解

[6]：关于P67，无监督学习和监督学习：
“概率的链式法，该分解意味着我们可以将其拆分成n个监督学习问题，来解决表面上的无监督学习p(x)。另外，我们求解监督学习问题p(y|x)时，也可以使用传统的无监督学习策略学习联合分布p（x,y）”
我：监督学习和无监督学习无清晰界限

[7]：关于P68，
“有时标签可能不止一个数，例如，如果我们想要训练语音模型转录整个句子。这里每个句子样本的标签是一个单词序列”
我：输出的维数可能不止一维

（2017-09-13）

[8]：关于P70，样本假设，
“通常，我们会做一系列被统称为独立同分布的假设。该假设是说，每个数据集中的样本都是彼此相互独立的，并且训练集和测试集是同分布的，采样自相同的分布”
我：这个几乎是所以机器学习算法，甚至深度学习算法对样本的假设？对吗？

[9]: 关于P71,
“以下是决定机器学习算法效果是否好的因素：
（1）	降低训练误差。
（2）	缩小训练误差和测试误差和测试误差的差距。
这两个因素对应机器学习的两个主要挑战：欠拟合和过拟合”
我：这是机器学习乃至深度学习的最终目标

[10]：关于P71，容量：
“通过调整模型的容量，我们可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集。容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质”
我：是不是指模型复杂度，比如网络的层数和宽度？

[11]：关于P72，VC维，
“统计学习理论提供了量化模型容量的不同方法。在这些方法中，最有名的是Vapnik-Chervovenkis 维度，简称VC维。VC维度量二元分类器的容量”
我：对VC维很不了解，需要加强，有了解的，可否分享？

[12]：关于P73，最邻近回归是否类似于聚类算法？

[13]：关于P73，
“对于非参数模型而言，更多的数据会得到更好的泛化能力，直到达到最佳可能的泛化误差。任何模型容量小于最有容量的固定参数模型会渐进大于贝叶斯误差的误差值。值得注意的是，具有最有容量的模型仍然有可能在训练误差和泛化误差之间存在很大的差距。在这种情况下，我们可以通过收集更多的训练样本来缩小差距”
我：模型容量对样本量的需求，样本量对降低模型误差的重要性。所以流程是：样本理解——模型选择（容量）——样本量——降低误差

[14]：关于P74，
“这意味着机器学习研究的目标不是找一个通用的学习算法或是绝对最好的学习算法，而是理解什么样的分布于人工智能获取经验的‘真实世界’相关，以及什么样的学习算法在我们关注的数据生成分布上效果最好”
我：case by case的问题

（2017-09-17）
